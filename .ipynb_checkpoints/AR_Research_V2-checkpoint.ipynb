{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.tsa.stattools as ts\n",
    "\n",
    "# 引入Kalman函数库\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "# 引入纯随机序列检测，只有是非纯随机序列，才能进行检测\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# 引入时序分析模型进行模型匹配\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一、读入读取的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据不同的文件名，读取相应的数据\n",
    "price_data = pd.read_csv('price0308.csv',index_col = 0)\n",
    "# nan处理\n",
    "price_data = price_data.fillna(0)\n",
    "# inf处理\n",
    "price_data[np.isinf(price_data)] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000001.XSHE</th>\n",
       "      <th>000002.XSHE</th>\n",
       "      <th>000063.XSHE</th>\n",
       "      <th>000069.XSHE</th>\n",
       "      <th>000100.XSHE</th>\n",
       "      <th>000157.XSHE</th>\n",
       "      <th>000166.XSHE</th>\n",
       "      <th>000333.XSHE</th>\n",
       "      <th>000338.XSHE</th>\n",
       "      <th>000402.XSHE</th>\n",
       "      <th>...</th>\n",
       "      <th>603156.XSHG</th>\n",
       "      <th>603160.XSHG</th>\n",
       "      <th>603259.XSHG</th>\n",
       "      <th>603260.XSHG</th>\n",
       "      <th>603288.XSHG</th>\n",
       "      <th>603799.XSHG</th>\n",
       "      <th>603833.XSHG</th>\n",
       "      <th>603858.XSHG</th>\n",
       "      <th>603986.XSHG</th>\n",
       "      <th>603993.XSHG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-15</th>\n",
       "      <td>10.95</td>\n",
       "      <td>26.70</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.72</td>\n",
       "      <td>3.20</td>\n",
       "      <td>4.11</td>\n",
       "      <td>4.85</td>\n",
       "      <td>55.22</td>\n",
       "      <td>8.01</td>\n",
       "      <td>8.73</td>\n",
       "      <td>...</td>\n",
       "      <td>68.66</td>\n",
       "      <td>73.45</td>\n",
       "      <td>50.08</td>\n",
       "      <td>73.21</td>\n",
       "      <td>69.99</td>\n",
       "      <td>83.37</td>\n",
       "      <td>146.11</td>\n",
       "      <td>37.26</td>\n",
       "      <td>116.79</td>\n",
       "      <td>8.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-16</th>\n",
       "      <td>10.74</td>\n",
       "      <td>26.20</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.69</td>\n",
       "      <td>3.17</td>\n",
       "      <td>4.07</td>\n",
       "      <td>4.79</td>\n",
       "      <td>54.40</td>\n",
       "      <td>7.98</td>\n",
       "      <td>8.73</td>\n",
       "      <td>...</td>\n",
       "      <td>69.04</td>\n",
       "      <td>72.07</td>\n",
       "      <td>55.09</td>\n",
       "      <td>73.87</td>\n",
       "      <td>72.09</td>\n",
       "      <td>82.35</td>\n",
       "      <td>146.26</td>\n",
       "      <td>40.53</td>\n",
       "      <td>120.64</td>\n",
       "      <td>8.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-17</th>\n",
       "      <td>10.66</td>\n",
       "      <td>25.77</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.67</td>\n",
       "      <td>3.13</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.75</td>\n",
       "      <td>53.12</td>\n",
       "      <td>8.11</td>\n",
       "      <td>8.86</td>\n",
       "      <td>...</td>\n",
       "      <td>66.75</td>\n",
       "      <td>72.25</td>\n",
       "      <td>60.60</td>\n",
       "      <td>75.29</td>\n",
       "      <td>68.53</td>\n",
       "      <td>80.99</td>\n",
       "      <td>145.99</td>\n",
       "      <td>39.79</td>\n",
       "      <td>121.54</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-18</th>\n",
       "      <td>10.79</td>\n",
       "      <td>26.12</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.75</td>\n",
       "      <td>3.15</td>\n",
       "      <td>4.08</td>\n",
       "      <td>4.78</td>\n",
       "      <td>53.89</td>\n",
       "      <td>8.16</td>\n",
       "      <td>9.02</td>\n",
       "      <td>...</td>\n",
       "      <td>65.99</td>\n",
       "      <td>72.91</td>\n",
       "      <td>66.66</td>\n",
       "      <td>74.46</td>\n",
       "      <td>68.71</td>\n",
       "      <td>81.85</td>\n",
       "      <td>146.52</td>\n",
       "      <td>39.38</td>\n",
       "      <td>119.41</td>\n",
       "      <td>8.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-21</th>\n",
       "      <td>10.78</td>\n",
       "      <td>26.04</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.77</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.83</td>\n",
       "      <td>53.95</td>\n",
       "      <td>8.08</td>\n",
       "      <td>9.09</td>\n",
       "      <td>...</td>\n",
       "      <td>65.44</td>\n",
       "      <td>74.64</td>\n",
       "      <td>73.33</td>\n",
       "      <td>78.57</td>\n",
       "      <td>69.64</td>\n",
       "      <td>82.33</td>\n",
       "      <td>145.67</td>\n",
       "      <td>39.60</td>\n",
       "      <td>131.35</td>\n",
       "      <td>8.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-22</th>\n",
       "      <td>10.70</td>\n",
       "      <td>25.71</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.74</td>\n",
       "      <td>3.18</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.81</td>\n",
       "      <td>52.63</td>\n",
       "      <td>8.17</td>\n",
       "      <td>9.00</td>\n",
       "      <td>...</td>\n",
       "      <td>66.32</td>\n",
       "      <td>74.46</td>\n",
       "      <td>80.66</td>\n",
       "      <td>79.39</td>\n",
       "      <td>69.71</td>\n",
       "      <td>83.32</td>\n",
       "      <td>148.22</td>\n",
       "      <td>39.12</td>\n",
       "      <td>128.87</td>\n",
       "      <td>8.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-23</th>\n",
       "      <td>10.49</td>\n",
       "      <td>25.78</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.61</td>\n",
       "      <td>3.16</td>\n",
       "      <td>4.06</td>\n",
       "      <td>4.78</td>\n",
       "      <td>51.80</td>\n",
       "      <td>8.14</td>\n",
       "      <td>8.91</td>\n",
       "      <td>...</td>\n",
       "      <td>65.29</td>\n",
       "      <td>73.38</td>\n",
       "      <td>88.73</td>\n",
       "      <td>80.81</td>\n",
       "      <td>70.53</td>\n",
       "      <td>79.76</td>\n",
       "      <td>148.68</td>\n",
       "      <td>39.31</td>\n",
       "      <td>128.27</td>\n",
       "      <td>7.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-24</th>\n",
       "      <td>10.45</td>\n",
       "      <td>25.64</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.62</td>\n",
       "      <td>3.14</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.71</td>\n",
       "      <td>50.90</td>\n",
       "      <td>8.01</td>\n",
       "      <td>9.02</td>\n",
       "      <td>...</td>\n",
       "      <td>65.14</td>\n",
       "      <td>72.04</td>\n",
       "      <td>97.60</td>\n",
       "      <td>80.63</td>\n",
       "      <td>69.39</td>\n",
       "      <td>79.82</td>\n",
       "      <td>144.48</td>\n",
       "      <td>39.32</td>\n",
       "      <td>123.23</td>\n",
       "      <td>7.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-25</th>\n",
       "      <td>10.43</td>\n",
       "      <td>25.39</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.50</td>\n",
       "      <td>3.12</td>\n",
       "      <td>4.04</td>\n",
       "      <td>4.66</td>\n",
       "      <td>50.59</td>\n",
       "      <td>8.14</td>\n",
       "      <td>8.99</td>\n",
       "      <td>...</td>\n",
       "      <td>63.90</td>\n",
       "      <td>70.84</td>\n",
       "      <td>107.36</td>\n",
       "      <td>78.07</td>\n",
       "      <td>69.37</td>\n",
       "      <td>75.76</td>\n",
       "      <td>144.45</td>\n",
       "      <td>40.43</td>\n",
       "      <td>120.65</td>\n",
       "      <td>7.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-28</th>\n",
       "      <td>10.43</td>\n",
       "      <td>25.49</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.59</td>\n",
       "      <td>3.10</td>\n",
       "      <td>4.01</td>\n",
       "      <td>4.66</td>\n",
       "      <td>51.94</td>\n",
       "      <td>8.29</td>\n",
       "      <td>9.06</td>\n",
       "      <td>...</td>\n",
       "      <td>64.19</td>\n",
       "      <td>69.04</td>\n",
       "      <td>118.10</td>\n",
       "      <td>78.66</td>\n",
       "      <td>71.38</td>\n",
       "      <td>75.93</td>\n",
       "      <td>144.15</td>\n",
       "      <td>41.53</td>\n",
       "      <td>116.00</td>\n",
       "      <td>7.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-29</th>\n",
       "      <td>10.22</td>\n",
       "      <td>24.63</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.42</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3.98</td>\n",
       "      <td>4.62</td>\n",
       "      <td>51.49</td>\n",
       "      <td>8.21</td>\n",
       "      <td>9.02</td>\n",
       "      <td>...</td>\n",
       "      <td>67.01</td>\n",
       "      <td>69.44</td>\n",
       "      <td>129.91</td>\n",
       "      <td>75.83</td>\n",
       "      <td>71.33</td>\n",
       "      <td>76.27</td>\n",
       "      <td>140.69</td>\n",
       "      <td>38.99</td>\n",
       "      <td>113.20</td>\n",
       "      <td>7.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-30</th>\n",
       "      <td>9.93</td>\n",
       "      <td>23.70</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.41</td>\n",
       "      <td>3.05</td>\n",
       "      <td>3.89</td>\n",
       "      <td>4.51</td>\n",
       "      <td>50.71</td>\n",
       "      <td>8.01</td>\n",
       "      <td>8.84</td>\n",
       "      <td>...</td>\n",
       "      <td>68.01</td>\n",
       "      <td>67.39</td>\n",
       "      <td>120.37</td>\n",
       "      <td>73.63</td>\n",
       "      <td>73.62</td>\n",
       "      <td>73.47</td>\n",
       "      <td>142.00</td>\n",
       "      <td>37.49</td>\n",
       "      <td>109.71</td>\n",
       "      <td>6.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>10.03</td>\n",
       "      <td>24.62</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.52</td>\n",
       "      <td>3.11</td>\n",
       "      <td>3.97</td>\n",
       "      <td>4.58</td>\n",
       "      <td>52.91</td>\n",
       "      <td>8.29</td>\n",
       "      <td>8.97</td>\n",
       "      <td>...</td>\n",
       "      <td>72.73</td>\n",
       "      <td>70.66</td>\n",
       "      <td>129.70</td>\n",
       "      <td>76.75</td>\n",
       "      <td>74.94</td>\n",
       "      <td>74.26</td>\n",
       "      <td>142.09</td>\n",
       "      <td>39.05</td>\n",
       "      <td>108.45</td>\n",
       "      <td>7.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-01</th>\n",
       "      <td>10.04</td>\n",
       "      <td>25.01</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.49</td>\n",
       "      <td>3.10</td>\n",
       "      <td>4.01</td>\n",
       "      <td>4.60</td>\n",
       "      <td>52.15</td>\n",
       "      <td>8.23</td>\n",
       "      <td>8.98</td>\n",
       "      <td>...</td>\n",
       "      <td>70.70</td>\n",
       "      <td>70.97</td>\n",
       "      <td>132.87</td>\n",
       "      <td>73.89</td>\n",
       "      <td>74.02</td>\n",
       "      <td>73.87</td>\n",
       "      <td>137.17</td>\n",
       "      <td>38.35</td>\n",
       "      <td>104.18</td>\n",
       "      <td>7.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-04</th>\n",
       "      <td>10.12</td>\n",
       "      <td>26.26</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.54</td>\n",
       "      <td>3.13</td>\n",
       "      <td>4.01</td>\n",
       "      <td>4.56</td>\n",
       "      <td>53.99</td>\n",
       "      <td>8.51</td>\n",
       "      <td>9.12</td>\n",
       "      <td>...</td>\n",
       "      <td>71.08</td>\n",
       "      <td>70.49</td>\n",
       "      <td>120.90</td>\n",
       "      <td>66.62</td>\n",
       "      <td>77.92</td>\n",
       "      <td>68.94</td>\n",
       "      <td>140.63</td>\n",
       "      <td>37.99</td>\n",
       "      <td>105.93</td>\n",
       "      <td>6.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-05</th>\n",
       "      <td>10.11</td>\n",
       "      <td>26.45</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.60</td>\n",
       "      <td>3.20</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.59</td>\n",
       "      <td>54.97</td>\n",
       "      <td>8.53</td>\n",
       "      <td>9.12</td>\n",
       "      <td>...</td>\n",
       "      <td>72.14</td>\n",
       "      <td>71.55</td>\n",
       "      <td>119.78</td>\n",
       "      <td>71.17</td>\n",
       "      <td>79.91</td>\n",
       "      <td>71.93</td>\n",
       "      <td>142.48</td>\n",
       "      <td>38.87</td>\n",
       "      <td>111.68</td>\n",
       "      <td>7.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-06</th>\n",
       "      <td>9.99</td>\n",
       "      <td>25.97</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.54</td>\n",
       "      <td>3.18</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.59</td>\n",
       "      <td>54.74</td>\n",
       "      <td>8.44</td>\n",
       "      <td>9.09</td>\n",
       "      <td>...</td>\n",
       "      <td>72.98</td>\n",
       "      <td>72.76</td>\n",
       "      <td>124.06</td>\n",
       "      <td>71.38</td>\n",
       "      <td>80.94</td>\n",
       "      <td>74.22</td>\n",
       "      <td>140.96</td>\n",
       "      <td>38.61</td>\n",
       "      <td>111.10</td>\n",
       "      <td>7.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-07</th>\n",
       "      <td>10.21</td>\n",
       "      <td>26.35</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.60</td>\n",
       "      <td>3.18</td>\n",
       "      <td>4.01</td>\n",
       "      <td>4.62</td>\n",
       "      <td>54.30</td>\n",
       "      <td>8.58</td>\n",
       "      <td>9.23</td>\n",
       "      <td>...</td>\n",
       "      <td>69.88</td>\n",
       "      <td>70.71</td>\n",
       "      <td>119.03</td>\n",
       "      <td>71.99</td>\n",
       "      <td>80.72</td>\n",
       "      <td>74.43</td>\n",
       "      <td>143.04</td>\n",
       "      <td>38.49</td>\n",
       "      <td>108.33</td>\n",
       "      <td>7.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-08</th>\n",
       "      <td>9.97</td>\n",
       "      <td>25.70</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.45</td>\n",
       "      <td>3.13</td>\n",
       "      <td>3.94</td>\n",
       "      <td>4.59</td>\n",
       "      <td>54.66</td>\n",
       "      <td>8.40</td>\n",
       "      <td>9.10</td>\n",
       "      <td>...</td>\n",
       "      <td>68.84</td>\n",
       "      <td>71.74</td>\n",
       "      <td>118.46</td>\n",
       "      <td>73.05</td>\n",
       "      <td>80.38</td>\n",
       "      <td>71.98</td>\n",
       "      <td>141.98</td>\n",
       "      <td>37.74</td>\n",
       "      <td>104.70</td>\n",
       "      <td>7.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-11</th>\n",
       "      <td>9.89</td>\n",
       "      <td>26.27</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.52</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3.91</td>\n",
       "      <td>4.63</td>\n",
       "      <td>56.32</td>\n",
       "      <td>8.54</td>\n",
       "      <td>8.96</td>\n",
       "      <td>...</td>\n",
       "      <td>66.72</td>\n",
       "      <td>70.84</td>\n",
       "      <td>108.12</td>\n",
       "      <td>72.86</td>\n",
       "      <td>75.96</td>\n",
       "      <td>74.84</td>\n",
       "      <td>141.11</td>\n",
       "      <td>36.51</td>\n",
       "      <td>100.26</td>\n",
       "      <td>7.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-12</th>\n",
       "      <td>9.91</td>\n",
       "      <td>26.66</td>\n",
       "      <td>31.31</td>\n",
       "      <td>7.55</td>\n",
       "      <td>3.14</td>\n",
       "      <td>4.01</td>\n",
       "      <td>4.69</td>\n",
       "      <td>57.19</td>\n",
       "      <td>9.23</td>\n",
       "      <td>9.00</td>\n",
       "      <td>...</td>\n",
       "      <td>68.62</td>\n",
       "      <td>71.58</td>\n",
       "      <td>109.80</td>\n",
       "      <td>72.42</td>\n",
       "      <td>80.29</td>\n",
       "      <td>74.37</td>\n",
       "      <td>142.11</td>\n",
       "      <td>36.91</td>\n",
       "      <td>100.00</td>\n",
       "      <td>7.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-13</th>\n",
       "      <td>9.80</td>\n",
       "      <td>26.59</td>\n",
       "      <td>28.18</td>\n",
       "      <td>7.55</td>\n",
       "      <td>3.13</td>\n",
       "      <td>3.97</td>\n",
       "      <td>4.62</td>\n",
       "      <td>56.99</td>\n",
       "      <td>9.26</td>\n",
       "      <td>8.96</td>\n",
       "      <td>...</td>\n",
       "      <td>66.54</td>\n",
       "      <td>71.08</td>\n",
       "      <td>107.98</td>\n",
       "      <td>70.63</td>\n",
       "      <td>78.02</td>\n",
       "      <td>72.55</td>\n",
       "      <td>140.70</td>\n",
       "      <td>35.95</td>\n",
       "      <td>103.83</td>\n",
       "      <td>6.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-14</th>\n",
       "      <td>9.92</td>\n",
       "      <td>26.33</td>\n",
       "      <td>25.36</td>\n",
       "      <td>7.63</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.97</td>\n",
       "      <td>4.65</td>\n",
       "      <td>55.78</td>\n",
       "      <td>8.97</td>\n",
       "      <td>8.83</td>\n",
       "      <td>...</td>\n",
       "      <td>65.75</td>\n",
       "      <td>70.66</td>\n",
       "      <td>100.26</td>\n",
       "      <td>71.53</td>\n",
       "      <td>76.84</td>\n",
       "      <td>70.37</td>\n",
       "      <td>142.12</td>\n",
       "      <td>35.99</td>\n",
       "      <td>103.92</td>\n",
       "      <td>6.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-15</th>\n",
       "      <td>10.02</td>\n",
       "      <td>26.93</td>\n",
       "      <td>22.82</td>\n",
       "      <td>7.71</td>\n",
       "      <td>3.05</td>\n",
       "      <td>3.96</td>\n",
       "      <td>4.62</td>\n",
       "      <td>56.28</td>\n",
       "      <td>9.04</td>\n",
       "      <td>8.83</td>\n",
       "      <td>...</td>\n",
       "      <td>63.36</td>\n",
       "      <td>68.60</td>\n",
       "      <td>100.53</td>\n",
       "      <td>68.98</td>\n",
       "      <td>77.81</td>\n",
       "      <td>69.71</td>\n",
       "      <td>140.73</td>\n",
       "      <td>35.41</td>\n",
       "      <td>104.87</td>\n",
       "      <td>6.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-19</th>\n",
       "      <td>9.72</td>\n",
       "      <td>26.07</td>\n",
       "      <td>20.54</td>\n",
       "      <td>7.40</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.83</td>\n",
       "      <td>4.27</td>\n",
       "      <td>54.62</td>\n",
       "      <td>8.41</td>\n",
       "      <td>8.36</td>\n",
       "      <td>...</td>\n",
       "      <td>59.85</td>\n",
       "      <td>64.35</td>\n",
       "      <td>98.94</td>\n",
       "      <td>65.28</td>\n",
       "      <td>76.26</td>\n",
       "      <td>62.74</td>\n",
       "      <td>136.91</td>\n",
       "      <td>32.41</td>\n",
       "      <td>101.19</td>\n",
       "      <td>6.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-20</th>\n",
       "      <td>9.76</td>\n",
       "      <td>25.94</td>\n",
       "      <td>18.49</td>\n",
       "      <td>7.52</td>\n",
       "      <td>2.89</td>\n",
       "      <td>3.85</td>\n",
       "      <td>4.36</td>\n",
       "      <td>55.16</td>\n",
       "      <td>8.42</td>\n",
       "      <td>8.37</td>\n",
       "      <td>...</td>\n",
       "      <td>60.69</td>\n",
       "      <td>65.13</td>\n",
       "      <td>100.99</td>\n",
       "      <td>67.62</td>\n",
       "      <td>76.80</td>\n",
       "      <td>65.60</td>\n",
       "      <td>140.93</td>\n",
       "      <td>32.68</td>\n",
       "      <td>100.26</td>\n",
       "      <td>6.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-21</th>\n",
       "      <td>9.71</td>\n",
       "      <td>26.68</td>\n",
       "      <td>16.64</td>\n",
       "      <td>7.54</td>\n",
       "      <td>2.77</td>\n",
       "      <td>3.84</td>\n",
       "      <td>4.26</td>\n",
       "      <td>54.22</td>\n",
       "      <td>8.49</td>\n",
       "      <td>8.35</td>\n",
       "      <td>...</td>\n",
       "      <td>60.73</td>\n",
       "      <td>63.87</td>\n",
       "      <td>91.39</td>\n",
       "      <td>66.68</td>\n",
       "      <td>74.69</td>\n",
       "      <td>63.37</td>\n",
       "      <td>136.23</td>\n",
       "      <td>31.63</td>\n",
       "      <td>90.56</td>\n",
       "      <td>6.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-22</th>\n",
       "      <td>9.70</td>\n",
       "      <td>27.03</td>\n",
       "      <td>14.98</td>\n",
       "      <td>7.58</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.89</td>\n",
       "      <td>4.30</td>\n",
       "      <td>53.65</td>\n",
       "      <td>8.70</td>\n",
       "      <td>8.44</td>\n",
       "      <td>...</td>\n",
       "      <td>60.65</td>\n",
       "      <td>63.41</td>\n",
       "      <td>89.90</td>\n",
       "      <td>69.09</td>\n",
       "      <td>74.69</td>\n",
       "      <td>66.91</td>\n",
       "      <td>137.74</td>\n",
       "      <td>32.20</td>\n",
       "      <td>91.92</td>\n",
       "      <td>6.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-25</th>\n",
       "      <td>9.32</td>\n",
       "      <td>25.48</td>\n",
       "      <td>13.48</td>\n",
       "      <td>7.46</td>\n",
       "      <td>2.77</td>\n",
       "      <td>3.89</td>\n",
       "      <td>4.29</td>\n",
       "      <td>52.90</td>\n",
       "      <td>8.40</td>\n",
       "      <td>8.38</td>\n",
       "      <td>...</td>\n",
       "      <td>63.09</td>\n",
       "      <td>63.13</td>\n",
       "      <td>93.02</td>\n",
       "      <td>68.23</td>\n",
       "      <td>76.97</td>\n",
       "      <td>67.56</td>\n",
       "      <td>135.82</td>\n",
       "      <td>31.90</td>\n",
       "      <td>90.87</td>\n",
       "      <td>6.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-26</th>\n",
       "      <td>9.22</td>\n",
       "      <td>24.57</td>\n",
       "      <td>13.28</td>\n",
       "      <td>7.08</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.88</td>\n",
       "      <td>4.30</td>\n",
       "      <td>52.33</td>\n",
       "      <td>8.20</td>\n",
       "      <td>8.17</td>\n",
       "      <td>...</td>\n",
       "      <td>63.45</td>\n",
       "      <td>63.95</td>\n",
       "      <td>95.70</td>\n",
       "      <td>68.57</td>\n",
       "      <td>75.51</td>\n",
       "      <td>68.78</td>\n",
       "      <td>130.81</td>\n",
       "      <td>31.97</td>\n",
       "      <td>97.64</td>\n",
       "      <td>6.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-21</th>\n",
       "      <td>10.34</td>\n",
       "      <td>25.65</td>\n",
       "      <td>21.04</td>\n",
       "      <td>6.16</td>\n",
       "      <td>2.73</td>\n",
       "      <td>3.69</td>\n",
       "      <td>4.38</td>\n",
       "      <td>42.09</td>\n",
       "      <td>8.27</td>\n",
       "      <td>6.84</td>\n",
       "      <td>...</td>\n",
       "      <td>44.67</td>\n",
       "      <td>74.13</td>\n",
       "      <td>77.83</td>\n",
       "      <td>45.81</td>\n",
       "      <td>73.10</td>\n",
       "      <td>30.19</td>\n",
       "      <td>88.50</td>\n",
       "      <td>25.50</td>\n",
       "      <td>67.45</td>\n",
       "      <td>3.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22</th>\n",
       "      <td>10.28</td>\n",
       "      <td>25.40</td>\n",
       "      <td>20.44</td>\n",
       "      <td>6.12</td>\n",
       "      <td>2.71</td>\n",
       "      <td>3.70</td>\n",
       "      <td>4.34</td>\n",
       "      <td>41.30</td>\n",
       "      <td>8.20</td>\n",
       "      <td>6.75</td>\n",
       "      <td>...</td>\n",
       "      <td>43.97</td>\n",
       "      <td>74.66</td>\n",
       "      <td>76.09</td>\n",
       "      <td>45.55</td>\n",
       "      <td>72.20</td>\n",
       "      <td>29.38</td>\n",
       "      <td>86.97</td>\n",
       "      <td>25.18</td>\n",
       "      <td>66.00</td>\n",
       "      <td>3.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-23</th>\n",
       "      <td>10.35</td>\n",
       "      <td>25.31</td>\n",
       "      <td>20.45</td>\n",
       "      <td>6.13</td>\n",
       "      <td>2.72</td>\n",
       "      <td>3.68</td>\n",
       "      <td>4.32</td>\n",
       "      <td>41.46</td>\n",
       "      <td>8.33</td>\n",
       "      <td>6.77</td>\n",
       "      <td>...</td>\n",
       "      <td>43.58</td>\n",
       "      <td>74.10</td>\n",
       "      <td>75.69</td>\n",
       "      <td>45.55</td>\n",
       "      <td>71.99</td>\n",
       "      <td>29.80</td>\n",
       "      <td>88.88</td>\n",
       "      <td>25.07</td>\n",
       "      <td>65.00</td>\n",
       "      <td>3.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-24</th>\n",
       "      <td>10.52</td>\n",
       "      <td>25.41</td>\n",
       "      <td>20.37</td>\n",
       "      <td>6.17</td>\n",
       "      <td>2.72</td>\n",
       "      <td>3.72</td>\n",
       "      <td>4.38</td>\n",
       "      <td>41.59</td>\n",
       "      <td>8.33</td>\n",
       "      <td>6.79</td>\n",
       "      <td>...</td>\n",
       "      <td>43.67</td>\n",
       "      <td>74.50</td>\n",
       "      <td>76.66</td>\n",
       "      <td>46.40</td>\n",
       "      <td>72.45</td>\n",
       "      <td>29.49</td>\n",
       "      <td>86.76</td>\n",
       "      <td>25.09</td>\n",
       "      <td>68.20</td>\n",
       "      <td>3.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-25</th>\n",
       "      <td>11.00</td>\n",
       "      <td>26.10</td>\n",
       "      <td>20.32</td>\n",
       "      <td>6.30</td>\n",
       "      <td>2.72</td>\n",
       "      <td>3.74</td>\n",
       "      <td>4.38</td>\n",
       "      <td>41.90</td>\n",
       "      <td>8.60</td>\n",
       "      <td>6.85</td>\n",
       "      <td>...</td>\n",
       "      <td>43.59</td>\n",
       "      <td>74.97</td>\n",
       "      <td>76.38</td>\n",
       "      <td>46.05</td>\n",
       "      <td>72.98</td>\n",
       "      <td>29.24</td>\n",
       "      <td>90.95</td>\n",
       "      <td>24.62</td>\n",
       "      <td>69.72</td>\n",
       "      <td>3.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-28</th>\n",
       "      <td>10.94</td>\n",
       "      <td>26.06</td>\n",
       "      <td>20.24</td>\n",
       "      <td>6.27</td>\n",
       "      <td>2.74</td>\n",
       "      <td>3.69</td>\n",
       "      <td>4.39</td>\n",
       "      <td>41.84</td>\n",
       "      <td>8.56</td>\n",
       "      <td>6.86</td>\n",
       "      <td>...</td>\n",
       "      <td>43.19</td>\n",
       "      <td>75.13</td>\n",
       "      <td>74.47</td>\n",
       "      <td>45.80</td>\n",
       "      <td>72.59</td>\n",
       "      <td>28.79</td>\n",
       "      <td>92.53</td>\n",
       "      <td>24.27</td>\n",
       "      <td>69.90</td>\n",
       "      <td>3.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-29</th>\n",
       "      <td>11.00</td>\n",
       "      <td>26.88</td>\n",
       "      <td>19.98</td>\n",
       "      <td>6.41</td>\n",
       "      <td>2.72</td>\n",
       "      <td>3.73</td>\n",
       "      <td>4.39</td>\n",
       "      <td>43.00</td>\n",
       "      <td>8.80</td>\n",
       "      <td>7.09</td>\n",
       "      <td>...</td>\n",
       "      <td>43.34</td>\n",
       "      <td>74.30</td>\n",
       "      <td>74.52</td>\n",
       "      <td>47.20</td>\n",
       "      <td>72.59</td>\n",
       "      <td>26.71</td>\n",
       "      <td>92.49</td>\n",
       "      <td>23.98</td>\n",
       "      <td>69.20</td>\n",
       "      <td>3.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30</th>\n",
       "      <td>10.95</td>\n",
       "      <td>27.21</td>\n",
       "      <td>19.71</td>\n",
       "      <td>6.41</td>\n",
       "      <td>2.76</td>\n",
       "      <td>3.70</td>\n",
       "      <td>4.35</td>\n",
       "      <td>42.10</td>\n",
       "      <td>8.72</td>\n",
       "      <td>7.00</td>\n",
       "      <td>...</td>\n",
       "      <td>42.50</td>\n",
       "      <td>72.10</td>\n",
       "      <td>76.36</td>\n",
       "      <td>47.67</td>\n",
       "      <td>72.47</td>\n",
       "      <td>26.69</td>\n",
       "      <td>88.00</td>\n",
       "      <td>23.88</td>\n",
       "      <td>67.12</td>\n",
       "      <td>3.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>11.10</td>\n",
       "      <td>27.75</td>\n",
       "      <td>20.11</td>\n",
       "      <td>6.40</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.72</td>\n",
       "      <td>4.42</td>\n",
       "      <td>43.53</td>\n",
       "      <td>8.85</td>\n",
       "      <td>7.05</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>72.35</td>\n",
       "      <td>75.53</td>\n",
       "      <td>46.95</td>\n",
       "      <td>72.30</td>\n",
       "      <td>26.19</td>\n",
       "      <td>89.92</td>\n",
       "      <td>23.89</td>\n",
       "      <td>67.69</td>\n",
       "      <td>3.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01</th>\n",
       "      <td>11.20</td>\n",
       "      <td>27.68</td>\n",
       "      <td>20.41</td>\n",
       "      <td>6.40</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4.50</td>\n",
       "      <td>44.26</td>\n",
       "      <td>8.97</td>\n",
       "      <td>7.14</td>\n",
       "      <td>...</td>\n",
       "      <td>41.00</td>\n",
       "      <td>73.31</td>\n",
       "      <td>77.20</td>\n",
       "      <td>48.22</td>\n",
       "      <td>73.95</td>\n",
       "      <td>27.07</td>\n",
       "      <td>89.13</td>\n",
       "      <td>24.46</td>\n",
       "      <td>71.06</td>\n",
       "      <td>3.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-11</th>\n",
       "      <td>11.21</td>\n",
       "      <td>27.67</td>\n",
       "      <td>20.87</td>\n",
       "      <td>6.48</td>\n",
       "      <td>2.87</td>\n",
       "      <td>3.85</td>\n",
       "      <td>4.57</td>\n",
       "      <td>46.08</td>\n",
       "      <td>9.06</td>\n",
       "      <td>7.18</td>\n",
       "      <td>...</td>\n",
       "      <td>41.94</td>\n",
       "      <td>74.85</td>\n",
       "      <td>79.04</td>\n",
       "      <td>49.51</td>\n",
       "      <td>75.55</td>\n",
       "      <td>29.78</td>\n",
       "      <td>93.55</td>\n",
       "      <td>25.04</td>\n",
       "      <td>75.22</td>\n",
       "      <td>4.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-12</th>\n",
       "      <td>11.19</td>\n",
       "      <td>27.40</td>\n",
       "      <td>21.06</td>\n",
       "      <td>6.47</td>\n",
       "      <td>3.03</td>\n",
       "      <td>3.87</td>\n",
       "      <td>4.54</td>\n",
       "      <td>46.49</td>\n",
       "      <td>9.34</td>\n",
       "      <td>7.18</td>\n",
       "      <td>...</td>\n",
       "      <td>41.65</td>\n",
       "      <td>75.07</td>\n",
       "      <td>80.51</td>\n",
       "      <td>50.10</td>\n",
       "      <td>75.40</td>\n",
       "      <td>30.33</td>\n",
       "      <td>92.84</td>\n",
       "      <td>25.26</td>\n",
       "      <td>74.14</td>\n",
       "      <td>4.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-13</th>\n",
       "      <td>11.38</td>\n",
       "      <td>27.77</td>\n",
       "      <td>22.95</td>\n",
       "      <td>6.57</td>\n",
       "      <td>3.33</td>\n",
       "      <td>3.93</td>\n",
       "      <td>4.65</td>\n",
       "      <td>46.80</td>\n",
       "      <td>9.39</td>\n",
       "      <td>7.28</td>\n",
       "      <td>...</td>\n",
       "      <td>44.32</td>\n",
       "      <td>82.58</td>\n",
       "      <td>80.96</td>\n",
       "      <td>50.13</td>\n",
       "      <td>76.57</td>\n",
       "      <td>31.79</td>\n",
       "      <td>93.12</td>\n",
       "      <td>25.55</td>\n",
       "      <td>75.35</td>\n",
       "      <td>4.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-14</th>\n",
       "      <td>11.25</td>\n",
       "      <td>27.44</td>\n",
       "      <td>22.85</td>\n",
       "      <td>6.54</td>\n",
       "      <td>3.24</td>\n",
       "      <td>3.97</td>\n",
       "      <td>4.76</td>\n",
       "      <td>47.15</td>\n",
       "      <td>9.40</td>\n",
       "      <td>7.37</td>\n",
       "      <td>...</td>\n",
       "      <td>44.43</td>\n",
       "      <td>83.40</td>\n",
       "      <td>83.41</td>\n",
       "      <td>52.16</td>\n",
       "      <td>76.80</td>\n",
       "      <td>31.47</td>\n",
       "      <td>94.40</td>\n",
       "      <td>25.70</td>\n",
       "      <td>77.10</td>\n",
       "      <td>4.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-15</th>\n",
       "      <td>10.95</td>\n",
       "      <td>26.55</td>\n",
       "      <td>23.00</td>\n",
       "      <td>6.33</td>\n",
       "      <td>3.22</td>\n",
       "      <td>3.95</td>\n",
       "      <td>4.63</td>\n",
       "      <td>45.27</td>\n",
       "      <td>8.95</td>\n",
       "      <td>7.29</td>\n",
       "      <td>...</td>\n",
       "      <td>46.02</td>\n",
       "      <td>82.30</td>\n",
       "      <td>81.91</td>\n",
       "      <td>52.36</td>\n",
       "      <td>74.92</td>\n",
       "      <td>31.73</td>\n",
       "      <td>90.96</td>\n",
       "      <td>25.48</td>\n",
       "      <td>74.70</td>\n",
       "      <td>4.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-18</th>\n",
       "      <td>11.36</td>\n",
       "      <td>27.20</td>\n",
       "      <td>24.14</td>\n",
       "      <td>6.55</td>\n",
       "      <td>3.31</td>\n",
       "      <td>4.07</td>\n",
       "      <td>4.86</td>\n",
       "      <td>46.40</td>\n",
       "      <td>9.72</td>\n",
       "      <td>7.40</td>\n",
       "      <td>...</td>\n",
       "      <td>46.90</td>\n",
       "      <td>84.85</td>\n",
       "      <td>85.18</td>\n",
       "      <td>55.21</td>\n",
       "      <td>76.74</td>\n",
       "      <td>33.38</td>\n",
       "      <td>93.10</td>\n",
       "      <td>26.23</td>\n",
       "      <td>78.60</td>\n",
       "      <td>4.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-19</th>\n",
       "      <td>11.27</td>\n",
       "      <td>27.03</td>\n",
       "      <td>24.06</td>\n",
       "      <td>6.54</td>\n",
       "      <td>3.24</td>\n",
       "      <td>4.04</td>\n",
       "      <td>4.85</td>\n",
       "      <td>46.08</td>\n",
       "      <td>9.61</td>\n",
       "      <td>7.38</td>\n",
       "      <td>...</td>\n",
       "      <td>46.10</td>\n",
       "      <td>83.75</td>\n",
       "      <td>84.70</td>\n",
       "      <td>54.59</td>\n",
       "      <td>75.89</td>\n",
       "      <td>32.82</td>\n",
       "      <td>93.61</td>\n",
       "      <td>26.02</td>\n",
       "      <td>78.39</td>\n",
       "      <td>4.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-20</th>\n",
       "      <td>11.41</td>\n",
       "      <td>27.63</td>\n",
       "      <td>24.00</td>\n",
       "      <td>6.68</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.04</td>\n",
       "      <td>4.86</td>\n",
       "      <td>46.08</td>\n",
       "      <td>9.56</td>\n",
       "      <td>7.36</td>\n",
       "      <td>...</td>\n",
       "      <td>45.81</td>\n",
       "      <td>85.00</td>\n",
       "      <td>84.00</td>\n",
       "      <td>55.08</td>\n",
       "      <td>74.30</td>\n",
       "      <td>34.12</td>\n",
       "      <td>91.82</td>\n",
       "      <td>25.93</td>\n",
       "      <td>76.50</td>\n",
       "      <td>4.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-21</th>\n",
       "      <td>11.36</td>\n",
       "      <td>26.95</td>\n",
       "      <td>25.31</td>\n",
       "      <td>6.59</td>\n",
       "      <td>3.17</td>\n",
       "      <td>4.04</td>\n",
       "      <td>4.90</td>\n",
       "      <td>45.86</td>\n",
       "      <td>9.54</td>\n",
       "      <td>7.25</td>\n",
       "      <td>...</td>\n",
       "      <td>45.52</td>\n",
       "      <td>85.36</td>\n",
       "      <td>82.31</td>\n",
       "      <td>53.21</td>\n",
       "      <td>75.27</td>\n",
       "      <td>33.95</td>\n",
       "      <td>90.65</td>\n",
       "      <td>25.74</td>\n",
       "      <td>77.11</td>\n",
       "      <td>4.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-22</th>\n",
       "      <td>11.54</td>\n",
       "      <td>26.97</td>\n",
       "      <td>27.84</td>\n",
       "      <td>6.55</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4.10</td>\n",
       "      <td>5.39</td>\n",
       "      <td>45.61</td>\n",
       "      <td>9.50</td>\n",
       "      <td>7.38</td>\n",
       "      <td>...</td>\n",
       "      <td>45.76</td>\n",
       "      <td>86.52</td>\n",
       "      <td>85.28</td>\n",
       "      <td>56.11</td>\n",
       "      <td>75.88</td>\n",
       "      <td>34.25</td>\n",
       "      <td>91.36</td>\n",
       "      <td>26.08</td>\n",
       "      <td>81.21</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-25</th>\n",
       "      <td>12.55</td>\n",
       "      <td>28.39</td>\n",
       "      <td>30.31</td>\n",
       "      <td>6.95</td>\n",
       "      <td>3.45</td>\n",
       "      <td>4.25</td>\n",
       "      <td>5.93</td>\n",
       "      <td>49.17</td>\n",
       "      <td>9.92</td>\n",
       "      <td>7.77</td>\n",
       "      <td>...</td>\n",
       "      <td>47.33</td>\n",
       "      <td>91.73</td>\n",
       "      <td>88.73</td>\n",
       "      <td>57.51</td>\n",
       "      <td>78.40</td>\n",
       "      <td>37.55</td>\n",
       "      <td>98.70</td>\n",
       "      <td>27.40</td>\n",
       "      <td>89.33</td>\n",
       "      <td>4.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-26</th>\n",
       "      <td>12.20</td>\n",
       "      <td>27.79</td>\n",
       "      <td>29.90</td>\n",
       "      <td>6.93</td>\n",
       "      <td>3.42</td>\n",
       "      <td>4.22</td>\n",
       "      <td>5.73</td>\n",
       "      <td>47.36</td>\n",
       "      <td>9.60</td>\n",
       "      <td>7.90</td>\n",
       "      <td>...</td>\n",
       "      <td>46.07</td>\n",
       "      <td>88.91</td>\n",
       "      <td>87.58</td>\n",
       "      <td>56.50</td>\n",
       "      <td>76.47</td>\n",
       "      <td>39.62</td>\n",
       "      <td>96.99</td>\n",
       "      <td>27.96</td>\n",
       "      <td>93.11</td>\n",
       "      <td>4.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-27</th>\n",
       "      <td>12.40</td>\n",
       "      <td>28.01</td>\n",
       "      <td>29.45</td>\n",
       "      <td>6.99</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.19</td>\n",
       "      <td>5.73</td>\n",
       "      <td>47.64</td>\n",
       "      <td>9.48</td>\n",
       "      <td>8.02</td>\n",
       "      <td>...</td>\n",
       "      <td>45.45</td>\n",
       "      <td>85.70</td>\n",
       "      <td>86.79</td>\n",
       "      <td>55.49</td>\n",
       "      <td>76.34</td>\n",
       "      <td>38.45</td>\n",
       "      <td>97.90</td>\n",
       "      <td>27.65</td>\n",
       "      <td>92.60</td>\n",
       "      <td>4.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>12.36</td>\n",
       "      <td>27.99</td>\n",
       "      <td>29.80</td>\n",
       "      <td>6.92</td>\n",
       "      <td>3.23</td>\n",
       "      <td>4.20</td>\n",
       "      <td>5.50</td>\n",
       "      <td>47.78</td>\n",
       "      <td>9.56</td>\n",
       "      <td>8.00</td>\n",
       "      <td>...</td>\n",
       "      <td>45.86</td>\n",
       "      <td>85.45</td>\n",
       "      <td>89.88</td>\n",
       "      <td>56.99</td>\n",
       "      <td>76.68</td>\n",
       "      <td>38.22</td>\n",
       "      <td>94.00</td>\n",
       "      <td>28.04</td>\n",
       "      <td>92.00</td>\n",
       "      <td>4.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-01</th>\n",
       "      <td>12.76</td>\n",
       "      <td>28.54</td>\n",
       "      <td>29.73</td>\n",
       "      <td>6.99</td>\n",
       "      <td>3.29</td>\n",
       "      <td>4.23</td>\n",
       "      <td>5.64</td>\n",
       "      <td>48.57</td>\n",
       "      <td>10.03</td>\n",
       "      <td>8.01</td>\n",
       "      <td>...</td>\n",
       "      <td>45.88</td>\n",
       "      <td>86.31</td>\n",
       "      <td>89.89</td>\n",
       "      <td>56.44</td>\n",
       "      <td>77.56</td>\n",
       "      <td>38.50</td>\n",
       "      <td>95.07</td>\n",
       "      <td>28.00</td>\n",
       "      <td>92.21</td>\n",
       "      <td>4.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-04</th>\n",
       "      <td>12.99</td>\n",
       "      <td>29.70</td>\n",
       "      <td>30.31</td>\n",
       "      <td>7.17</td>\n",
       "      <td>3.45</td>\n",
       "      <td>4.31</td>\n",
       "      <td>5.66</td>\n",
       "      <td>48.96</td>\n",
       "      <td>10.65</td>\n",
       "      <td>8.15</td>\n",
       "      <td>...</td>\n",
       "      <td>46.15</td>\n",
       "      <td>88.90</td>\n",
       "      <td>96.66</td>\n",
       "      <td>56.45</td>\n",
       "      <td>78.60</td>\n",
       "      <td>38.52</td>\n",
       "      <td>101.90</td>\n",
       "      <td>28.38</td>\n",
       "      <td>96.71</td>\n",
       "      <td>4.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-05</th>\n",
       "      <td>13.06</td>\n",
       "      <td>29.94</td>\n",
       "      <td>31.65</td>\n",
       "      <td>7.25</td>\n",
       "      <td>3.80</td>\n",
       "      <td>4.38</td>\n",
       "      <td>5.64</td>\n",
       "      <td>49.00</td>\n",
       "      <td>10.90</td>\n",
       "      <td>8.31</td>\n",
       "      <td>...</td>\n",
       "      <td>45.96</td>\n",
       "      <td>93.95</td>\n",
       "      <td>99.34</td>\n",
       "      <td>56.20</td>\n",
       "      <td>78.00</td>\n",
       "      <td>38.45</td>\n",
       "      <td>108.48</td>\n",
       "      <td>28.75</td>\n",
       "      <td>106.38</td>\n",
       "      <td>4.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06</th>\n",
       "      <td>13.08</td>\n",
       "      <td>29.48</td>\n",
       "      <td>31.30</td>\n",
       "      <td>7.24</td>\n",
       "      <td>4.02</td>\n",
       "      <td>4.41</td>\n",
       "      <td>5.91</td>\n",
       "      <td>49.10</td>\n",
       "      <td>10.72</td>\n",
       "      <td>8.34</td>\n",
       "      <td>...</td>\n",
       "      <td>47.78</td>\n",
       "      <td>93.69</td>\n",
       "      <td>98.28</td>\n",
       "      <td>57.10</td>\n",
       "      <td>77.30</td>\n",
       "      <td>39.64</td>\n",
       "      <td>108.00</td>\n",
       "      <td>29.28</td>\n",
       "      <td>106.95</td>\n",
       "      <td>5.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-07</th>\n",
       "      <td>12.74</td>\n",
       "      <td>28.57</td>\n",
       "      <td>30.40</td>\n",
       "      <td>7.17</td>\n",
       "      <td>3.91</td>\n",
       "      <td>4.39</td>\n",
       "      <td>6.15</td>\n",
       "      <td>47.56</td>\n",
       "      <td>10.54</td>\n",
       "      <td>8.23</td>\n",
       "      <td>...</td>\n",
       "      <td>47.48</td>\n",
       "      <td>92.40</td>\n",
       "      <td>95.38</td>\n",
       "      <td>57.30</td>\n",
       "      <td>75.66</td>\n",
       "      <td>40.17</td>\n",
       "      <td>104.31</td>\n",
       "      <td>29.11</td>\n",
       "      <td>104.50</td>\n",
       "      <td>5.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-08</th>\n",
       "      <td>12.30</td>\n",
       "      <td>27.40</td>\n",
       "      <td>29.21</td>\n",
       "      <td>6.83</td>\n",
       "      <td>3.63</td>\n",
       "      <td>4.17</td>\n",
       "      <td>5.54</td>\n",
       "      <td>45.80</td>\n",
       "      <td>10.30</td>\n",
       "      <td>7.83</td>\n",
       "      <td>...</td>\n",
       "      <td>45.68</td>\n",
       "      <td>95.20</td>\n",
       "      <td>93.56</td>\n",
       "      <td>55.00</td>\n",
       "      <td>74.85</td>\n",
       "      <td>37.63</td>\n",
       "      <td>100.57</td>\n",
       "      <td>27.70</td>\n",
       "      <td>105.90</td>\n",
       "      <td>4.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            000001.XSHE  000002.XSHE  000063.XSHE  000069.XSHE  000100.XSHE  \\\n",
       "2018-05-15        10.95        26.70        31.31         7.72         3.20   \n",
       "2018-05-16        10.74        26.20        31.31         7.69         3.17   \n",
       "2018-05-17        10.66        25.77        31.31         7.67         3.13   \n",
       "2018-05-18        10.79        26.12        31.31         7.75         3.15   \n",
       "2018-05-21        10.78        26.04        31.31         7.77         3.19   \n",
       "2018-05-22        10.70        25.71        31.31         7.74         3.18   \n",
       "2018-05-23        10.49        25.78        31.31         7.61         3.16   \n",
       "2018-05-24        10.45        25.64        31.31         7.62         3.14   \n",
       "2018-05-25        10.43        25.39        31.31         7.50         3.12   \n",
       "2018-05-28        10.43        25.49        31.31         7.59         3.10   \n",
       "2018-05-29        10.22        24.63        31.31         7.42         3.12   \n",
       "2018-05-30         9.93        23.70        31.31         7.41         3.05   \n",
       "2018-05-31        10.03        24.62        31.31         7.52         3.11   \n",
       "2018-06-01        10.04        25.01        31.31         7.49         3.10   \n",
       "2018-06-04        10.12        26.26        31.31         7.54         3.13   \n",
       "2018-06-05        10.11        26.45        31.31         7.60         3.20   \n",
       "2018-06-06         9.99        25.97        31.31         7.54         3.18   \n",
       "2018-06-07        10.21        26.35        31.31         7.60         3.18   \n",
       "2018-06-08         9.97        25.70        31.31         7.45         3.13   \n",
       "2018-06-11         9.89        26.27        31.31         7.52         3.12   \n",
       "2018-06-12         9.91        26.66        31.31         7.55         3.14   \n",
       "2018-06-13         9.80        26.59        28.18         7.55         3.13   \n",
       "2018-06-14         9.92        26.33        25.36         7.63         3.08   \n",
       "2018-06-15        10.02        26.93        22.82         7.71         3.05   \n",
       "2018-06-19         9.72        26.07        20.54         7.40         2.90   \n",
       "2018-06-20         9.76        25.94        18.49         7.52         2.89   \n",
       "2018-06-21         9.71        26.68        16.64         7.54         2.77   \n",
       "2018-06-22         9.70        27.03        14.98         7.58         2.80   \n",
       "2018-06-25         9.32        25.48        13.48         7.46         2.77   \n",
       "2018-06-26         9.22        24.57        13.28         7.08         2.84   \n",
       "...                 ...          ...          ...          ...          ...   \n",
       "2019-01-21        10.34        25.65        21.04         6.16         2.73   \n",
       "2019-01-22        10.28        25.40        20.44         6.12         2.71   \n",
       "2019-01-23        10.35        25.31        20.45         6.13         2.72   \n",
       "2019-01-24        10.52        25.41        20.37         6.17         2.72   \n",
       "2019-01-25        11.00        26.10        20.32         6.30         2.72   \n",
       "2019-01-28        10.94        26.06        20.24         6.27         2.74   \n",
       "2019-01-29        11.00        26.88        19.98         6.41         2.72   \n",
       "2019-01-30        10.95        27.21        19.71         6.41         2.76   \n",
       "2019-01-31        11.10        27.75        20.11         6.40         2.75   \n",
       "2019-02-01        11.20        27.68        20.41         6.40         2.84   \n",
       "2019-02-11        11.21        27.67        20.87         6.48         2.87   \n",
       "2019-02-12        11.19        27.40        21.06         6.47         3.03   \n",
       "2019-02-13        11.38        27.77        22.95         6.57         3.33   \n",
       "2019-02-14        11.25        27.44        22.85         6.54         3.24   \n",
       "2019-02-15        10.95        26.55        23.00         6.33         3.22   \n",
       "2019-02-18        11.36        27.20        24.14         6.55         3.31   \n",
       "2019-02-19        11.27        27.03        24.06         6.54         3.24   \n",
       "2019-02-20        11.41        27.63        24.00         6.68         3.25   \n",
       "2019-02-21        11.36        26.95        25.31         6.59         3.17   \n",
       "2019-02-22        11.54        26.97        27.84         6.55         3.19   \n",
       "2019-02-25        12.55        28.39        30.31         6.95         3.45   \n",
       "2019-02-26        12.20        27.79        29.90         6.93         3.42   \n",
       "2019-02-27        12.40        28.01        29.45         6.99         3.25   \n",
       "2019-02-28        12.36        27.99        29.80         6.92         3.23   \n",
       "2019-03-01        12.76        28.54        29.73         6.99         3.29   \n",
       "2019-03-04        12.99        29.70        30.31         7.17         3.45   \n",
       "2019-03-05        13.06        29.94        31.65         7.25         3.80   \n",
       "2019-03-06        13.08        29.48        31.30         7.24         4.02   \n",
       "2019-03-07        12.74        28.57        30.40         7.17         3.91   \n",
       "2019-03-08        12.30        27.40        29.21         6.83         3.63   \n",
       "\n",
       "            000157.XSHE  000166.XSHE  000333.XSHE  000338.XSHE  000402.XSHE  \\\n",
       "2018-05-15         4.11         4.85        55.22         8.01         8.73   \n",
       "2018-05-16         4.07         4.79        54.40         7.98         8.73   \n",
       "2018-05-17         4.05         4.75        53.12         8.11         8.86   \n",
       "2018-05-18         4.08         4.78        53.89         8.16         9.02   \n",
       "2018-05-21         4.10         4.83        53.95         8.08         9.09   \n",
       "2018-05-22         4.10         4.81        52.63         8.17         9.00   \n",
       "2018-05-23         4.06         4.78        51.80         8.14         8.91   \n",
       "2018-05-24         4.05         4.71        50.90         8.01         9.02   \n",
       "2018-05-25         4.04         4.66        50.59         8.14         8.99   \n",
       "2018-05-28         4.01         4.66        51.94         8.29         9.06   \n",
       "2018-05-29         3.98         4.62        51.49         8.21         9.02   \n",
       "2018-05-30         3.89         4.51        50.71         8.01         8.84   \n",
       "2018-05-31         3.97         4.58        52.91         8.29         8.97   \n",
       "2018-06-01         4.01         4.60        52.15         8.23         8.98   \n",
       "2018-06-04         4.01         4.56        53.99         8.51         9.12   \n",
       "2018-06-05         4.00         4.59        54.97         8.53         9.12   \n",
       "2018-06-06         4.00         4.59        54.74         8.44         9.09   \n",
       "2018-06-07         4.01         4.62        54.30         8.58         9.23   \n",
       "2018-06-08         3.94         4.59        54.66         8.40         9.10   \n",
       "2018-06-11         3.91         4.63        56.32         8.54         8.96   \n",
       "2018-06-12         4.01         4.69        57.19         9.23         9.00   \n",
       "2018-06-13         3.97         4.62        56.99         9.26         8.96   \n",
       "2018-06-14         3.97         4.65        55.78         8.97         8.83   \n",
       "2018-06-15         3.96         4.62        56.28         9.04         8.83   \n",
       "2018-06-19         3.83         4.27        54.62         8.41         8.36   \n",
       "2018-06-20         3.85         4.36        55.16         8.42         8.37   \n",
       "2018-06-21         3.84         4.26        54.22         8.49         8.35   \n",
       "2018-06-22         3.89         4.30        53.65         8.70         8.44   \n",
       "2018-06-25         3.89         4.29        52.90         8.40         8.38   \n",
       "2018-06-26         3.88         4.30        52.33         8.20         8.17   \n",
       "...                 ...          ...          ...          ...          ...   \n",
       "2019-01-21         3.69         4.38        42.09         8.27         6.84   \n",
       "2019-01-22         3.70         4.34        41.30         8.20         6.75   \n",
       "2019-01-23         3.68         4.32        41.46         8.33         6.77   \n",
       "2019-01-24         3.72         4.38        41.59         8.33         6.79   \n",
       "2019-01-25         3.74         4.38        41.90         8.60         6.85   \n",
       "2019-01-28         3.69         4.39        41.84         8.56         6.86   \n",
       "2019-01-29         3.73         4.39        43.00         8.80         7.09   \n",
       "2019-01-30         3.70         4.35        42.10         8.72         7.00   \n",
       "2019-01-31         3.72         4.42        43.53         8.85         7.05   \n",
       "2019-02-01         3.75         4.50        44.26         8.97         7.14   \n",
       "2019-02-11         3.85         4.57        46.08         9.06         7.18   \n",
       "2019-02-12         3.87         4.54        46.49         9.34         7.18   \n",
       "2019-02-13         3.93         4.65        46.80         9.39         7.28   \n",
       "2019-02-14         3.97         4.76        47.15         9.40         7.37   \n",
       "2019-02-15         3.95         4.63        45.27         8.95         7.29   \n",
       "2019-02-18         4.07         4.86        46.40         9.72         7.40   \n",
       "2019-02-19         4.04         4.85        46.08         9.61         7.38   \n",
       "2019-02-20         4.04         4.86        46.08         9.56         7.36   \n",
       "2019-02-21         4.04         4.90        45.86         9.54         7.25   \n",
       "2019-02-22         4.10         5.39        45.61         9.50         7.38   \n",
       "2019-02-25         4.25         5.93        49.17         9.92         7.77   \n",
       "2019-02-26         4.22         5.73        47.36         9.60         7.90   \n",
       "2019-02-27         4.19         5.73        47.64         9.48         8.02   \n",
       "2019-02-28         4.20         5.50        47.78         9.56         8.00   \n",
       "2019-03-01         4.23         5.64        48.57        10.03         8.01   \n",
       "2019-03-04         4.31         5.66        48.96        10.65         8.15   \n",
       "2019-03-05         4.38         5.64        49.00        10.90         8.31   \n",
       "2019-03-06         4.41         5.91        49.10        10.72         8.34   \n",
       "2019-03-07         4.39         6.15        47.56        10.54         8.23   \n",
       "2019-03-08         4.17         5.54        45.80        10.30         7.83   \n",
       "\n",
       "               ...       603156.XSHG  603160.XSHG  603259.XSHG  603260.XSHG  \\\n",
       "2018-05-15     ...             68.66        73.45        50.08        73.21   \n",
       "2018-05-16     ...             69.04        72.07        55.09        73.87   \n",
       "2018-05-17     ...             66.75        72.25        60.60        75.29   \n",
       "2018-05-18     ...             65.99        72.91        66.66        74.46   \n",
       "2018-05-21     ...             65.44        74.64        73.33        78.57   \n",
       "2018-05-22     ...             66.32        74.46        80.66        79.39   \n",
       "2018-05-23     ...             65.29        73.38        88.73        80.81   \n",
       "2018-05-24     ...             65.14        72.04        97.60        80.63   \n",
       "2018-05-25     ...             63.90        70.84       107.36        78.07   \n",
       "2018-05-28     ...             64.19        69.04       118.10        78.66   \n",
       "2018-05-29     ...             67.01        69.44       129.91        75.83   \n",
       "2018-05-30     ...             68.01        67.39       120.37        73.63   \n",
       "2018-05-31     ...             72.73        70.66       129.70        76.75   \n",
       "2018-06-01     ...             70.70        70.97       132.87        73.89   \n",
       "2018-06-04     ...             71.08        70.49       120.90        66.62   \n",
       "2018-06-05     ...             72.14        71.55       119.78        71.17   \n",
       "2018-06-06     ...             72.98        72.76       124.06        71.38   \n",
       "2018-06-07     ...             69.88        70.71       119.03        71.99   \n",
       "2018-06-08     ...             68.84        71.74       118.46        73.05   \n",
       "2018-06-11     ...             66.72        70.84       108.12        72.86   \n",
       "2018-06-12     ...             68.62        71.58       109.80        72.42   \n",
       "2018-06-13     ...             66.54        71.08       107.98        70.63   \n",
       "2018-06-14     ...             65.75        70.66       100.26        71.53   \n",
       "2018-06-15     ...             63.36        68.60       100.53        68.98   \n",
       "2018-06-19     ...             59.85        64.35        98.94        65.28   \n",
       "2018-06-20     ...             60.69        65.13       100.99        67.62   \n",
       "2018-06-21     ...             60.73        63.87        91.39        66.68   \n",
       "2018-06-22     ...             60.65        63.41        89.90        69.09   \n",
       "2018-06-25     ...             63.09        63.13        93.02        68.23   \n",
       "2018-06-26     ...             63.45        63.95        95.70        68.57   \n",
       "...            ...               ...          ...          ...          ...   \n",
       "2019-01-21     ...             44.67        74.13        77.83        45.81   \n",
       "2019-01-22     ...             43.97        74.66        76.09        45.55   \n",
       "2019-01-23     ...             43.58        74.10        75.69        45.55   \n",
       "2019-01-24     ...             43.67        74.50        76.66        46.40   \n",
       "2019-01-25     ...             43.59        74.97        76.38        46.05   \n",
       "2019-01-28     ...             43.19        75.13        74.47        45.80   \n",
       "2019-01-29     ...             43.34        74.30        74.52        47.20   \n",
       "2019-01-30     ...             42.50        72.10        76.36        47.67   \n",
       "2019-01-31     ...             38.25        72.35        75.53        46.95   \n",
       "2019-02-01     ...             41.00        73.31        77.20        48.22   \n",
       "2019-02-11     ...             41.94        74.85        79.04        49.51   \n",
       "2019-02-12     ...             41.65        75.07        80.51        50.10   \n",
       "2019-02-13     ...             44.32        82.58        80.96        50.13   \n",
       "2019-02-14     ...             44.43        83.40        83.41        52.16   \n",
       "2019-02-15     ...             46.02        82.30        81.91        52.36   \n",
       "2019-02-18     ...             46.90        84.85        85.18        55.21   \n",
       "2019-02-19     ...             46.10        83.75        84.70        54.59   \n",
       "2019-02-20     ...             45.81        85.00        84.00        55.08   \n",
       "2019-02-21     ...             45.52        85.36        82.31        53.21   \n",
       "2019-02-22     ...             45.76        86.52        85.28        56.11   \n",
       "2019-02-25     ...             47.33        91.73        88.73        57.51   \n",
       "2019-02-26     ...             46.07        88.91        87.58        56.50   \n",
       "2019-02-27     ...             45.45        85.70        86.79        55.49   \n",
       "2019-02-28     ...             45.86        85.45        89.88        56.99   \n",
       "2019-03-01     ...             45.88        86.31        89.89        56.44   \n",
       "2019-03-04     ...             46.15        88.90        96.66        56.45   \n",
       "2019-03-05     ...             45.96        93.95        99.34        56.20   \n",
       "2019-03-06     ...             47.78        93.69        98.28        57.10   \n",
       "2019-03-07     ...             47.48        92.40        95.38        57.30   \n",
       "2019-03-08     ...             45.68        95.20        93.56        55.00   \n",
       "\n",
       "            603288.XSHG  603799.XSHG  603833.XSHG  603858.XSHG  603986.XSHG  \\\n",
       "2018-05-15        69.99        83.37       146.11        37.26       116.79   \n",
       "2018-05-16        72.09        82.35       146.26        40.53       120.64   \n",
       "2018-05-17        68.53        80.99       145.99        39.79       121.54   \n",
       "2018-05-18        68.71        81.85       146.52        39.38       119.41   \n",
       "2018-05-21        69.64        82.33       145.67        39.60       131.35   \n",
       "2018-05-22        69.71        83.32       148.22        39.12       128.87   \n",
       "2018-05-23        70.53        79.76       148.68        39.31       128.27   \n",
       "2018-05-24        69.39        79.82       144.48        39.32       123.23   \n",
       "2018-05-25        69.37        75.76       144.45        40.43       120.65   \n",
       "2018-05-28        71.38        75.93       144.15        41.53       116.00   \n",
       "2018-05-29        71.33        76.27       140.69        38.99       113.20   \n",
       "2018-05-30        73.62        73.47       142.00        37.49       109.71   \n",
       "2018-05-31        74.94        74.26       142.09        39.05       108.45   \n",
       "2018-06-01        74.02        73.87       137.17        38.35       104.18   \n",
       "2018-06-04        77.92        68.94       140.63        37.99       105.93   \n",
       "2018-06-05        79.91        71.93       142.48        38.87       111.68   \n",
       "2018-06-06        80.94        74.22       140.96        38.61       111.10   \n",
       "2018-06-07        80.72        74.43       143.04        38.49       108.33   \n",
       "2018-06-08        80.38        71.98       141.98        37.74       104.70   \n",
       "2018-06-11        75.96        74.84       141.11        36.51       100.26   \n",
       "2018-06-12        80.29        74.37       142.11        36.91       100.00   \n",
       "2018-06-13        78.02        72.55       140.70        35.95       103.83   \n",
       "2018-06-14        76.84        70.37       142.12        35.99       103.92   \n",
       "2018-06-15        77.81        69.71       140.73        35.41       104.87   \n",
       "2018-06-19        76.26        62.74       136.91        32.41       101.19   \n",
       "2018-06-20        76.80        65.60       140.93        32.68       100.26   \n",
       "2018-06-21        74.69        63.37       136.23        31.63        90.56   \n",
       "2018-06-22        74.69        66.91       137.74        32.20        91.92   \n",
       "2018-06-25        76.97        67.56       135.82        31.90        90.87   \n",
       "2018-06-26        75.51        68.78       130.81        31.97        97.64   \n",
       "...                 ...          ...          ...          ...          ...   \n",
       "2019-01-21        73.10        30.19        88.50        25.50        67.45   \n",
       "2019-01-22        72.20        29.38        86.97        25.18        66.00   \n",
       "2019-01-23        71.99        29.80        88.88        25.07        65.00   \n",
       "2019-01-24        72.45        29.49        86.76        25.09        68.20   \n",
       "2019-01-25        72.98        29.24        90.95        24.62        69.72   \n",
       "2019-01-28        72.59        28.79        92.53        24.27        69.90   \n",
       "2019-01-29        72.59        26.71        92.49        23.98        69.20   \n",
       "2019-01-30        72.47        26.69        88.00        23.88        67.12   \n",
       "2019-01-31        72.30        26.19        89.92        23.89        67.69   \n",
       "2019-02-01        73.95        27.07        89.13        24.46        71.06   \n",
       "2019-02-11        75.55        29.78        93.55        25.04        75.22   \n",
       "2019-02-12        75.40        30.33        92.84        25.26        74.14   \n",
       "2019-02-13        76.57        31.79        93.12        25.55        75.35   \n",
       "2019-02-14        76.80        31.47        94.40        25.70        77.10   \n",
       "2019-02-15        74.92        31.73        90.96        25.48        74.70   \n",
       "2019-02-18        76.74        33.38        93.10        26.23        78.60   \n",
       "2019-02-19        75.89        32.82        93.61        26.02        78.39   \n",
       "2019-02-20        74.30        34.12        91.82        25.93        76.50   \n",
       "2019-02-21        75.27        33.95        90.65        25.74        77.11   \n",
       "2019-02-22        75.88        34.25        91.36        26.08        81.21   \n",
       "2019-02-25        78.40        37.55        98.70        27.40        89.33   \n",
       "2019-02-26        76.47        39.62        96.99        27.96        93.11   \n",
       "2019-02-27        76.34        38.45        97.90        27.65        92.60   \n",
       "2019-02-28        76.68        38.22        94.00        28.04        92.00   \n",
       "2019-03-01        77.56        38.50        95.07        28.00        92.21   \n",
       "2019-03-04        78.60        38.52       101.90        28.38        96.71   \n",
       "2019-03-05        78.00        38.45       108.48        28.75       106.38   \n",
       "2019-03-06        77.30        39.64       108.00        29.28       106.95   \n",
       "2019-03-07        75.66        40.17       104.31        29.11       104.50   \n",
       "2019-03-08        74.85        37.63       100.57        27.70       105.90   \n",
       "\n",
       "            603993.XSHG  \n",
       "2018-05-15         8.22  \n",
       "2018-05-16         8.08  \n",
       "2018-05-17         8.00  \n",
       "2018-05-18         8.09  \n",
       "2018-05-21         8.16  \n",
       "2018-05-22         8.01  \n",
       "2018-05-23         7.70  \n",
       "2018-05-24         7.67  \n",
       "2018-05-25         7.43  \n",
       "2018-05-28         7.43  \n",
       "2018-05-29         7.37  \n",
       "2018-05-30         6.92  \n",
       "2018-05-31         7.02  \n",
       "2018-06-01         7.08  \n",
       "2018-06-04         6.93  \n",
       "2018-06-05         7.09  \n",
       "2018-06-06         7.22  \n",
       "2018-06-07         7.14  \n",
       "2018-06-08         7.01  \n",
       "2018-06-11         7.04  \n",
       "2018-06-12         7.02  \n",
       "2018-06-13         6.87  \n",
       "2018-06-14         6.87  \n",
       "2018-06-15         6.74  \n",
       "2018-06-19         6.07  \n",
       "2018-06-20         6.16  \n",
       "2018-06-21         6.02  \n",
       "2018-06-22         6.22  \n",
       "2018-06-25         6.20  \n",
       "2018-06-26         6.30  \n",
       "...                 ...  \n",
       "2019-01-21         3.92  \n",
       "2019-01-22         3.86  \n",
       "2019-01-23         3.89  \n",
       "2019-01-24         3.86  \n",
       "2019-01-25         3.85  \n",
       "2019-01-28         3.84  \n",
       "2019-01-29         3.79  \n",
       "2019-01-30         3.79  \n",
       "2019-01-31         3.84  \n",
       "2019-02-01         3.90  \n",
       "2019-02-11         4.06  \n",
       "2019-02-12         4.05  \n",
       "2019-02-13         4.19  \n",
       "2019-02-14         4.16  \n",
       "2019-02-15         4.16  \n",
       "2019-02-18         4.32  \n",
       "2019-02-19         4.31  \n",
       "2019-02-20         4.42  \n",
       "2019-02-21         4.34  \n",
       "2019-02-22         4.39  \n",
       "2019-02-25         4.78  \n",
       "2019-02-26         4.92  \n",
       "2019-02-27         4.80  \n",
       "2019-02-28         4.76  \n",
       "2019-03-01         4.79  \n",
       "2019-03-04         4.83  \n",
       "2019-03-05         4.88  \n",
       "2019-03-06         5.07  \n",
       "2019-03-07         5.15  \n",
       "2019-03-08         4.76  \n",
       "\n",
       "[200 rows x 300 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二、检验残差的平稳性、均值回归性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数名：Cadf_test\n",
    "# 输入参数：\n",
    "# 1、res_pd:pandas数组，index为日期，列名为“res”\n",
    "# 输出参数：\n",
    "# P value：返回test的p值，用于后续监测\n",
    "# null hypothesis of the Augmented Dickey-Fuller is that there is a unit root\n",
    "\n",
    "def Cadf_test(res_pd):\n",
    "\n",
    "    # 使用adf计算adf的值\n",
    "    cadf = ts.adfuller(res_pd)\n",
    "    \n",
    "    return cadf[1]\n",
    "\n",
    "\n",
    "# 重要：只有时间序列不是一个白噪声（纯随机序列）的时候，该序列才可做分析\n",
    "# 函数名：test_stochastic\n",
    "# 输入参数：\n",
    "# 1、res_pd:pandas数组，index为日期，列名为“res”\n",
    "# 输出参数：\n",
    "# P value：返回test的p值，用于后续监测\n",
    "# Ljung-Box test for no autocorrelation\n",
    "# 纯随机性检验,p值小于5%,序列为非白噪声\n",
    "# H0: 原本的数据都是纯随机序列\n",
    "# 用于检验某个时间段内的一系列观测值是不是随机的独立观测值\n",
    "# 如果观测值并非彼此独立，一个观测值可能会在 i 个时间单位后与另一个观测值相关，形成一种称为自相关的关系\n",
    "# 自相关可以削减基于时间的预测模型（例如时间序列图）的准确性，并导致数据的错误解释。\n",
    "\n",
    "def test_stochastic(ts):\n",
    "    p_value = acorr_ljungbox(ts)[1] #lags可自定义\n",
    "    return p_value[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三、逐个的寻找配对的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到配对序列：000001.XSHE，000063.XSHE\n",
      "找到配对序列：000001.XSHE，000408.XSHE\n",
      "找到配对序列：000001.XSHE，000413.XSHE\n",
      "找到配对序列：000001.XSHE，000415.XSHE\n",
      "找到配对序列：000001.XSHE，000630.XSHE\n",
      "找到配对序列：000001.XSHE，000725.XSHE\n",
      "找到配对序列：000001.XSHE，000839.XSHE\n",
      "找到配对序列：000001.XSHE，000959.XSHE\n",
      "找到配对序列：000001.XSHE，000963.XSHE\n",
      "找到配对序列：000001.XSHE，002032.XSHE\n",
      "找到配对序列：000001.XSHE，002236.XSHE\n",
      "找到配对序列：000001.XSHE，002252.XSHE\n",
      "找到配对序列：000001.XSHE，002310.XSHE\n",
      "找到配对序列：000001.XSHE，002411.XSHE\n",
      "找到配对序列：000001.XSHE，002450.XSHE\n",
      "找到配对序列：000001.XSHE，002508.XSHE\n",
      "找到配对序列：000001.XSHE，002736.XSHE\n",
      "找到配对序列：000001.XSHE，300003.XSHE\n",
      "找到配对序列：000001.XSHE，300017.XSHE\n",
      "找到配对序列：000001.XSHE，300033.XSHE\n",
      "找到配对序列：000001.XSHE，300072.XSHE\n",
      "找到配对序列：000001.XSHE，300433.XSHE\n",
      "找到配对序列：000001.XSHE，600221.XSHG\n",
      "找到配对序列：000001.XSHE，600297.XSHG\n",
      "找到配对序列：000001.XSHE，600339.XSHG\n",
      "找到配对序列：000001.XSHE，600518.XSHG\n",
      "找到配对序列：000001.XSHE，600739.XSHG\n",
      "找到配对序列：000001.XSHE，600816.XSHG\n",
      "找到配对序列：000001.XSHE，600837.XSHG\n",
      "找到配对序列：000001.XSHE，600887.XSHG\n",
      "找到配对序列：000001.XSHE，601021.XSHG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\pandas\\core\\algorithms.py:1823: RuntimeWarning: invalid value encountered in subtract\n",
      "  out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]\n"
     ]
    },
    {
     "ename": "MissingDataError",
     "evalue": "exog contains inf or nans",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingDataError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-49718a2601bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mValidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mValidation_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mCadf_factor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCadf_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mValidation_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mstochastic_factor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_stochastic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mValidation_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-ebcd429d4efd>\u001b[0m in \u001b[0;36mCadf_test\u001b[1;34m(res_pd)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# 使用adf计算adf的值\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mcadf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madfuller\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_pd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcadf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\statsmodels\\tsa\\stattools.pyc\u001b[0m in \u001b[0;36madfuller\u001b[1;34m(x, maxlag, regression, autolag, store, regresults)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mregresults\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             icbest, bestlag = _autolag(OLS, xdshort, fullRHS, startlag,\n\u001b[1;32m--> 241\u001b[1;33m                                        maxlag, autolag)\n\u001b[0m\u001b[0;32m    242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m             icbest, bestlag, alres = _autolag(OLS, xdshort, fullRHS, startlag,\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\statsmodels\\tsa\\stattools.pyc\u001b[0m in \u001b[0;36m_autolag\u001b[1;34m(mod, endog, exog, startlag, maxlag, method, modargs, fitargs, regresults)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstartlag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstartlag\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmaxlag\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mmod_instance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mlag\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlag\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod_instance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\statsmodels\\regression\\linear_model.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    815\u001b[0m                  **kwargs):\n\u001b[0;32m    816\u001b[0m         super(OLS, self).__init__(endog, exog, missing=missing,\n\u001b[1;32m--> 817\u001b[1;33m                                   hasconst=hasconst, **kwargs)\n\u001b[0m\u001b[0;32m    818\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"weights\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_keys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"weights\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\statsmodels\\regression\\linear_model.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    661\u001b[0m             \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m         super(WLS, self).__init__(endog, exog, missing=missing,\n\u001b[1;32m--> 663\u001b[1;33m                                   weights=weights, hasconst=hasconst, **kwargs)\n\u001b[0m\u001b[0;32m    664\u001b[0m         \u001b[0mnobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\statsmodels\\regression\\linear_model.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \"\"\"\n\u001b[0;32m    178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRegressionModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_attr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pinv_wexog'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wendog'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wexog'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\statsmodels\\base\\model.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLikelihoodModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\statsmodels\\base\\model.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mhasconst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hasconst'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         self.data = self._handle_data(endog, exog, missing, hasconst,\n\u001b[1;32m---> 64\u001b[1;33m                                       **kwargs)\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk_constant\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\statsmodels\\base\\model.pyc\u001b[0m in \u001b[0;36m_handle_data\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_handle_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;31m# kwargs arrays could have changed, easier to just attach here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\statsmodels\\base\\data.pyc\u001b[0m in \u001b[0;36mhandle_data\u001b[1;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    631\u001b[0m     \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle_data_class_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m     return klass(endog, exog=exog, missing=missing, hasconst=hasconst,\n\u001b[1;32m--> 633\u001b[1;33m                  **kwargs)\n\u001b[0m",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\statsmodels\\base\\data.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# this has side-effects, attaches k_constant and const_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_constant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhasconst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresettable_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\statsmodels\\base\\data.pyc\u001b[0m in \u001b[0;36m_handle_constant\u001b[1;34m(self, hasconst)\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0mptp_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mptp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mptp_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mMissingDataError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'exog contains inf or nans'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m             \u001b[0mconst_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mptp_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconst_idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMissingDataError\u001b[0m: exog contains inf or nans"
     ]
    }
   ],
   "source": [
    "data = price_data\n",
    "\n",
    "n = data.shape[1]\n",
    "\n",
    "keys = data.keys()\n",
    "pairs = []\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        S1 = data[keys[i]]\n",
    "        S2 = data[keys[j]]\n",
    "\n",
    "        # 构建检验序列\n",
    "        Validation_data = np.log(S1/S2).diff()\n",
    "\n",
    "        Validation_data = Validation_data.fillna(0)\n",
    "        \n",
    "        Cadf_factor = Cadf_test(Validation_data)\n",
    "        stochastic_factor = test_stochastic(Validation_data)\n",
    "        \n",
    "        if  (Cadf_factor < 0.05) and  (stochastic_factor < 0.05):\n",
    "            print \"找到配对序列：%s，%s\"%(str(keys[i]),str(keys[j]))\n",
    "            pairs.append([keys[i],keys[j],Cadf_factor,stochastic_factor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储相应的配对股票数据\n",
    "pairs_pd = pd.DataFrame(columns=['pairs1','pairs2','adf_factor','stochastic_factor'],data=pairs)\n",
    "pairs_pd.to_csv(\"pairs_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一、挑选沪深300指数，与沪深300指数对比，利用kalman方程求出残差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_beta(sec1 = '000858.XSHE' ,sec2 = '000300.XSHG',count = 400, end_date = '2015-3-1'):\n",
    "    # 赋初值\n",
    "    secs = [sec1, sec2]\n",
    "    ncount = count\n",
    "    end_date = end_date\n",
    "    \n",
    "    # 获取价格数据\n",
    "    data = get_price(secs, count =  ncount, end_date= end_date, frequency='1d', fields='close',fq = \"pre\")['close']\n",
    "    data.index.name = 'Date'\n",
    "    \n",
    "    # 观察矩阵\n",
    "    # 注意：\n",
    "    # 1、观察到的是sec1数据，sec1是自变量x，sec2是因变量y\n",
    "    # 2、需要使用add_constant来模拟alpha\n",
    "    # 3、需要使用np.newaxis来增加维度\n",
    "    obs_mat = sm.add_constant(data[secs[0]].values, prepend=False)[:, np.newaxis]\n",
    "\n",
    "    kf = KalmanFilter(n_dim_obs=1, n_dim_state=2, # y is 1-dimensional, (alpha, beta) is 2-dimensional\n",
    "                  initial_state_mean=np.ones(2),\n",
    "                  initial_state_covariance=np.ones((2, 2)),\n",
    "                  transition_matrices=np.eye(2),  # 不发生变化，都是单位矩阵\n",
    "                  observation_matrices=obs_mat,   # 观察矩阵\n",
    "                  observation_covariance=10**2,\n",
    "                  transition_covariance=0.01**2 * np.eye(2))\n",
    "    \n",
    "    # 相当于使用sec2来进行训练，模拟出beta、alpha\n",
    "    state_means, state_covs = kf.filter(data[secs[1]][:, np.newaxis])\n",
    "    return state_means[-1]\n",
    "\n",
    "\n",
    "# 函数名：Res_cal\n",
    "# 输入参数：\n",
    "# 1、sec1：比对的第一支股票\n",
    "# 2、sec2：比对的第二只股票\n",
    "# 3、ncount：样本数\n",
    "# 4、end_date：终止时间\n",
    "# 输出参数：\n",
    "# 序列：\n",
    "# 输出残差序列\n",
    "\n",
    "def Res_cal(sec1,sec2,ncount,end_date):\n",
    "    # 将股票改造成数组，并对参数赋初值\n",
    "    secs = [sec1, sec2]\n",
    "    ncount = ncount\n",
    "    end_date = end_date\n",
    "    \n",
    "    # 获得价格数据\n",
    "    p_data = get_price(secs, count =  ncount, end_date= end_date, frequency='1d', fields='close',fq = \"pre\")['close']\n",
    "\n",
    "    # 基于kalman方程获得alpha、beta\n",
    "    beta_kf = kalman_beta(sec1 = sec1 ,sec2 = sec2,count = ncount, end_date = end_date)\n",
    "    beta = beta_kf[0]\n",
    "    alpha = beta_kf[1]\n",
    "\n",
    "    # 利用alpha、beta计算两个股票的残差\n",
    "    # 注意：\n",
    "    # 1、股票1与股票2的顺序，sec2 = sec1 * beta + alpha\n",
    "    # 2、beta、alpha均包含在beta_kf中\n",
    "    res_pd = pd.DataFrame(p_data[secs[1]]- np.dot(sm.add_constant(p_data[secs[0]], prepend=False), beta_kf))\n",
    "    res_pd.columns = ['res']\n",
    "  \n",
    "    # 无效值填充\n",
    "    res_pd = res_pd.fillna(0)\n",
    "    \n",
    "\n",
    "    return res_pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成残差序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pd = Res_cal(sec1 = '000858.XSHE' ,sec2 = '000300.XSHG',ncount = 400, end_date = '2018-3-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二、检验残差的平稳性、均值回归性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数名：Cadf_test\n",
    "# 输入参数：\n",
    "# 1、res_pd:pandas数组，index为日期，列名为“res”\n",
    "# 输出参数：\n",
    "# P value：返回test的p值，用于后续监测\n",
    "# null hypothesis of the Augmented Dickey-Fuller is that there is a unit root\n",
    "\n",
    "def Cadf_test(res_pd):\n",
    "\n",
    "    # 使用adf计算adf的值\n",
    "    cadf = ts.adfuller(res_pd[\"res\"])\n",
    "    \n",
    "    return cadf[1]\n",
    "\n",
    "\n",
    "# 重要：只有时间序列不是一个白噪声（纯随机序列）的时候，该序列才可做分析\n",
    "# 函数名：test_stochastic\n",
    "# 输入参数：\n",
    "# 1、res_pd:pandas数组，index为日期，列名为“res”\n",
    "# 输出参数：\n",
    "# P value：返回test的p值，用于后续监测\n",
    "# Ljung-Box test for no autocorrelation\n",
    "# 纯随机性检验,p值小于5%,序列为非白噪声\n",
    "# H0: 原本的数据都是纯随机序列\n",
    "# 用于检验某个时间段内的一系列观测值是不是随机的独立观测值\n",
    "# 如果观测值并非彼此独立，一个观测值可能会在 i 个时间单位后与另一个观测值相关，形成一种称为自相关的关系\n",
    "# 自相关可以削减基于时间的预测模型（例如时间序列图）的准确性，并导致数据的错误解释。\n",
    "\n",
    "def test_stochastic(ts):\n",
    "    p_value = acorr_ljungbox(ts)[1] #lags可自定义\n",
    "    return p_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60446096098810431"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意选取时要截取后面一段，确保准确性\n",
    "Cadf_test(res_pd.iloc[-200:-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7588933829765305e-42"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stochastic(res_pd.iloc[-200:-10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三、确定ARMA的阶数\n",
    "\n",
    "ARMA(p,q)是AR(p)和MA(q)模型的组合，关于p和q的选择，一种方法是观察自相关图ACF和偏相关图PACF, 另一种方法是通过借助AIC、BIC统计量自动确定。由于我有几千个时间序列需要分别预测，所以选取自动的方式，而BIC可以有效应对模型的过拟合，因而选定BIC作为判断标准。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proper_model(data_ts, maxLag): \n",
    "    init_bic = float(\"inf\")\n",
    "    init_p = 0\n",
    "    init_q = 0\n",
    "    init_properModel = None\n",
    "    for p in np.arange(maxLag):\n",
    "        for q in np.arange(maxLag):\n",
    "            model = ARMA(data_ts, order=(p, q))\n",
    "            try:\n",
    "                results_ARMA = model.fit(disp=-1, method='css')\n",
    "            except:\n",
    "                continue\n",
    "            bic = results_ARMA.bic\n",
    "            if bic < init_bic:\n",
    "                init_p = p\n",
    "                init_q = q\n",
    "                init_properModel = results_ARMA\n",
    "                init_bic = bic\n",
    "    return init_bic, init_p, init_q, init_properModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:473: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  'available', HessianInversionWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n",
      "/opt/conda/envs/python2/lib/python2.7/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "model_para = proper_model(res_pd.iloc[-200:-10], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "四、拟合ARAM\n",
    "\n",
    "对于差分后的时间序列，运用于ARMA时该模型就被称为ARMIA，在代码层面改写为model = ARIMA(timeseries, order=(p,d,q))，但是实际上，用差分过的序列直接进行ARMA建模更方便，之后添加一步还原的操作即可。\n",
    "\n",
    "参考：\n",
    "1. 用statsmodel这个包来进行预测，很奇怪的是我从来没成功过，只能进行下一步（之后一天）的预测，多天的就无法做到了\n",
    "2. predict_ts = result_arma.predict(start=val.loc[0,'date'], end=val.loc[val.shape[0]-1,'date']) ，利用这个方法可以正确预测出一段时间范围内的结果\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2096.5525118413502,\n",
       " 9,\n",
       " 0,\n",
       " <statsmodels.tsa.arima_model.ARMAResultsWrapper at 0x7f3fa2812c10>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = model_para[3]\n",
    "\n",
    "output = model_fit.forecast(steps=5, exog=None, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 264.2499769 ,  252.6526545 ,  315.86473891,  322.51634883,\n",
       "         347.45482915]),\n",
       " array([  67.67234559,   94.13513324,  114.14819259,  129.52957898,\n",
       "         146.57082903]),\n",
       " array([[ 131.6146168 ,  396.88533701],\n",
       "        [  68.15118368,  437.15412532],\n",
       "        [  92.13839253,  539.59108529],\n",
       "        [  68.64303911,  576.38965856],\n",
       "        [  60.18128308,  634.72837523]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-02-08</th>\n",
       "      <td>244.877026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-09</th>\n",
       "      <td>115.029382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-12</th>\n",
       "      <td>1.386388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-13</th>\n",
       "      <td>11.077482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-14</th>\n",
       "      <td>-33.944971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-22</th>\n",
       "      <td>-126.850097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-23</th>\n",
       "      <td>-85.116898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-26</th>\n",
       "      <td>19.347590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-27</th>\n",
       "      <td>83.525847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>141.159241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01</th>\n",
       "      <td>164.531623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   res\n",
       "2018-02-08  244.877026\n",
       "2018-02-09  115.029382\n",
       "2018-02-12    1.386388\n",
       "2018-02-13   11.077482\n",
       "2018-02-14  -33.944971\n",
       "2018-02-22 -126.850097\n",
       "2018-02-23  -85.116898\n",
       "2018-02-26   19.347590\n",
       "2018-02-27   83.525847\n",
       "2018-02-28  141.159241\n",
       "2018-03-01  164.531623"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_pd.iloc[-11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
